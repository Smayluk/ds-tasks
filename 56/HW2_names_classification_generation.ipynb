{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Домашнее задание по NLP # 3 [100 баллов] \n",
    "## Классификация и генерация фамилий \n",
    "\n",
    "В этом домашнем задании вам предстоит классифицировать и генерировать фамилии на 19 разных языках. Обучающие данные хранятся в папке data и разбиты по языкам: один язык – одна подпапка. Ниже представлен код для считывания данных в словарь вида: \n",
    "```d{язык} : [список имен]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/English.txt', 'data/names/French.txt', 'data/names/German.txt', 'data/names/Greek.txt', 'data/names/Irish.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Spanish.txt', 'data/names/Vietnamese.txt']\n",
      "['Abels', 'Abelsky', 'Abeltsev', 'Abelyan', 'Aberson', 'Abertasov', 'Abesadze', 'Abezgauz', 'Abgaryan', 'Abibulaev', 'Abidoff', 'Abidov', 'Abih', 'Abikh', 'Abisaloff', 'Abisalov', 'Abitoff', 'Abitov', 'Abjaliloff', 'Abjalilov']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "all_categories = []\n",
    "category_lines = {}\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(category_lines['Russian'][100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предварительная обработка данных [10 баллов]\n",
    "\n",
    "1. Удалите неоднозначные фамилии (т.е. одинаковые фамилии на разных языка), если такие есть;\n",
    "2. Оцените \n",
    "* среднюю длину фамилии по всей коллекции\n",
    "* по каждому языку\n",
    "3. Для последующей классификации (части 2 и 3) оздайте обучающее и тестовое множество так, чтобы в обучающем множестве классы были сбалансированы: то есть, в обучающее множество должно входить примерно одинаковое количество фамилий на разных ящыка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic 2000\n",
      "Chinese 268\n",
      "Czech 519\n",
      "Dutch 297\n",
      "English 3668\n",
      "French 277\n",
      "German 724\n",
      "Greek 203\n",
      "Irish 232\n",
      "Italian 709\n",
      "Japanese 991\n",
      "Korean 94\n",
      "Polish 139\n",
      "Portuguese 74\n",
      "Russian 9408\n",
      "Scottish 100\n",
      "Spanish 298\n",
      "Vietnamese 73\n"
     ]
    }
   ],
   "source": [
    "for key in category_lines:\n",
    "    print(key, len(category_lines[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала попробуем поработать только с русскими и английскими фамилиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian = category_lines[\"Russian\"]\n",
    "english = category_lines[\"English\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_cleared = [x for x in russian if x not in english]\n",
    "english_cleared = [x for x in english if x not in russian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(russian_cleared), len(english_cleared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3641"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian = russian_cleared[:min_len]\n",
    "english = english_cleared[:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(russian)\n",
    "np.random.shuffle(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gulamov', 'Eideman', 'Filippovich', 'Baburkin', 'Bessogonov', 'Agababoff', 'Aweritchkin', 'Glasko', 'Awinowitski', 'Baikowski']\n",
      "russian len:  3641\n",
      "['Hunt', 'Graves', 'Donohoe', 'Conroy', 'Friel', 'Booth', 'Arnall', 'Eglan', 'Flowers', 'Allwood']\n",
      "english len:  3641\n"
     ]
    }
   ],
   "source": [
    "print(russian[:10])\n",
    "print('russian len: ', len(russian))\n",
    "print(english[:10])\n",
    "print('english len: ', len(english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2912\n"
     ]
    }
   ],
   "source": [
    "coef = int(min_len * 0.8)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(min_len)\n",
    "ones = np.ones(min_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим английские и русские фамилии, одновременно присвоив им метку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gulamov' '0.0']\n",
      " ['Eideman' '0.0']\n",
      " ['Filippovich' '0.0']\n",
      " ['Baburkin' '0.0']\n",
      " ['Bessogonov' '0.0']\n",
      " ['Agababoff' '0.0']\n",
      " ['Aweritchkin' '0.0']\n",
      " ['Glasko' '0.0']\n",
      " ['Awinowitski' '0.0']\n",
      " ['Baikowski' '0.0']]\n",
      "(2912, 2)\n",
      "[['Bezrukikh' '0.0']\n",
      " ['Glazanov' '0.0']\n",
      " ['Darevsky' '0.0']\n",
      " ['Harin' '0.0']\n",
      " ['Donchak' '0.0']\n",
      " ['Hananaev' '0.0']\n",
      " ['Grushikhin' '0.0']\n",
      " ['Emelin' '0.0']\n",
      " ['Gorsun' '0.0']\n",
      " ['Bajutkin' '0.0']]\n",
      "(729, 2)\n"
     ]
    }
   ],
   "source": [
    "russian_train = np.vstack((russian[:coef], zeros[:coef])).T\n",
    "print(russian_train[:10])\n",
    "print(russian_train.shape)\n",
    "russian_test = np.vstack((russian[coef : ], zeros[coef : ])).T\n",
    "print(russian_test[:10])\n",
    "print(russian_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hunt' '1.0']\n",
      " ['Graves' '1.0']\n",
      " ['Donohoe' '1.0']\n",
      " ['Conroy' '1.0']\n",
      " ['Friel' '1.0']\n",
      " ['Booth' '1.0']\n",
      " ['Arnall' '1.0']\n",
      " ['Eglan' '1.0']\n",
      " ['Flowers' '1.0']\n",
      " ['Allwood' '1.0']]\n",
      "(2912, 2)\n",
      "[['Lilwall' '1.0']\n",
      " ['Redding' '1.0']\n",
      " ['Reeves' '1.0']\n",
      " ['Levin' '1.0']\n",
      " ['Pearson' '1.0']\n",
      " ['Bassett' '1.0']\n",
      " ['Mcneil' '1.0']\n",
      " ['Keeler' '1.0']\n",
      " ['Jenas' '1.0']\n",
      " ['Eggison' '1.0']]\n",
      "(729, 2)\n"
     ]
    }
   ],
   "source": [
    "english_train = np.vstack((english[:coef], ones[:coef])).T\n",
    "print(english_train[:10])\n",
    "print(english_train.shape)\n",
    "english_test = np.vstack((english[coef: ], ones[coef : ])).T\n",
    "print(english_test[-10:])\n",
    "print(english_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Evert' '0.0']\n",
      " ['Eikhenvald' '0.0']\n",
      " ['Belosludtsev' '0.0']\n",
      " ['Abolins' '0.0']\n",
      " ['Avdakoff' '0.0']\n",
      " ['Dobretsov' '0.0']\n",
      " ['Djunusov' '0.0']\n",
      " ['Egamberdiev' '0.0']\n",
      " ['Alshits' '0.0']\n",
      " ['Abdulhabiroff' '0.0']\n",
      " ['Hunt' '1.0']\n",
      " ['Graves' '1.0']\n",
      " ['Donohoe' '1.0']\n",
      " ['Conroy' '1.0']\n",
      " ['Friel' '1.0']\n",
      " ['Booth' '1.0']\n",
      " ['Arnall' '1.0']\n",
      " ['Eglan' '1.0']\n",
      " ['Flowers' '1.0']\n",
      " ['Allwood' '1.0']]\n",
      "(5824, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.append(russian_train, english_train, axis=0)\n",
    "X_test = np.append(russian_test, english_test, axis=0)\n",
    "print(X_train[coef - 10 : coef + 10])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз перемешаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bethell' '1.0']\n",
      " ['Perez' '1.0']\n",
      " ['Dikson' '0.0']\n",
      " ['Rogan' '1.0']\n",
      " ['Hamitov' '0.0']\n",
      " ['Agar' '1.0']\n",
      " ['Gaskov' '0.0']\n",
      " ['Amod' '1.0']\n",
      " ['Azhogin' '0.0']\n",
      " ['Pomphrey' '1.0']\n",
      " ['Gudzenko' '0.0']\n",
      " ['Besschetny' '0.0']\n",
      " ['Ashby' '1.0']\n",
      " ['Gladkov' '0.0']\n",
      " ['Black' '1.0']\n",
      " ['Kay' '1.0']\n",
      " ['Doherty' '1.0']\n",
      " ['Hairutdinov' '0.0']\n",
      " ['Gorchinsky' '0.0']\n",
      " ['Thonon' '1.0']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[coef - 10 : coef + 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 2. Базовый метод классификации [20 баллов]\n",
    "\n",
    "\n",
    "\n",
    "Используйте метод наивного Байеса, логистическую регрессию или любой другой метод для классификации фамилий: в качестве признаков используйте символьные $n$-граммы. Сравните результаты, получаемые при разных $n=2,3,4$ по $F$-мере и аккуратности. В каких случаях метод ошибается?\n",
    "\n",
    "Для генерации $n$-грамм используйте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Создадим массив содержащий в строках фамилии в виде биграмм:\n",
    "- Фамилия - Предложение\n",
    "- Биграмма - Фамилия в предложении)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def get_ngrams(train, ngrams_count):\n",
    "    sequences = []\n",
    "    for words, label in train[:]:\n",
    "        seq = list(words.lower())\n",
    "        bigrams = ngrams(seq, ngrams_count)\n",
    "        bi = []\n",
    "        for grams in bigrams:           \n",
    "            if len(grams) >= ngrams_count:\n",
    "                bi.append(\"\".join(grams))\n",
    "        sequences.append([\" \".join(bi), int(float(label))])\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "def metrics_test(predicted, target):\n",
    "    acc = accuracy_score(predicted, target)\n",
    "    f1_micro = f1_score(predicted, target, average = 'micro')\n",
    "    f1_macro = f1_score(predicted, target, average = 'macro')\n",
    "    print('acc={0:1.4f}'.format(acc))\n",
    "    print('micro F1={0:1.4f}'.format(f1_micro))\n",
    "    print('macro F1={0:1.4f}'.format(f1_macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gram = get_ngrams(X_train, 2)\n",
    "X_test_gram = get_ngrams(X_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5824, 510)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.9129\n",
      "micro F1=0.9129\n",
      "macro F1=0.9129\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gram = get_ngrams(X_train, ngrams_count=3)\n",
    "X_test_gram = get_ngrams(X_test, ngrams_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5824, 3928)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.9335\n",
      "micro F1=0.9335\n",
      "macro F1=0.9334\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gram = get_ngrams(X_train, ngrams_count=4)\n",
    "X_test_gram = get_ngrams(X_test, ngrams_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5824, 10200)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.8903\n",
      "micro F1=0.8903\n",
      "macro F1=0.8897\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 3. Нейронная сеть [35 баллов]\n",
    "\n",
    "\n",
    "Используйте  реккурентную нейронную сеть с  LSTM для решения задачи. В ней может быть несколько слоев с LSTM, несколько слоев c Bidirectional(LSTM).  У нейронной сети один выход, определяющий класс фамилии. \n",
    "\n",
    "Представление имени для классификации в этом случае: \n",
    "1 вариант: бинарная матрица размера (количество букв в корпусе $\\times$ максимальная длина имени). Обозначим его через $x$. Если первая буква имени a, то $x[1][1] = 1$, если вторая – b, то  $x[2][1] = 1$. То есть, используем one hot encoding для векторизации букв.  \n",
    "2 вариант: Embedding'и символов. \n",
    "\n",
    "Выберите тот вариант, который вам проще или интереснее реализовать :) \n",
    "\n",
    "Не забудьте про регуляризацию нейронной сети дропаутами. \n",
    "\n",
    "Сравните результаты классификации разными методами по accuracy, micro- и macro- F-measure, precision, recall. Какой метод лучше и почему?\n",
    "\n",
    "Сравните результаты, получаемые при разных значениях дропаута, разных числах узлов на слоях нейронной сети по $F$-мере и аккуратности. В каких случаях нейронная сеть ошибается?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если совсем не получается запрограммировать нейронную сеть самостоятельно, обратитесь к туториалу тут: https://github.com/divamgupta/lstm-gender-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Посчитаем максимальные значения для того что бы понять размерности входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEntries = X_train.shape[0]\n",
    "maxlen = len(max( X_train[:, 0] , key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздадим таблицы для перевода символов в индексы и обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(  \"\".join(X_train[:, 0])  )\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total entries  5824\n",
      "max len  18\n",
      "total chars: 54\n"
     ]
    }
   ],
   "source": [
    "print(\"total entries \" , totalEntries)\n",
    "print(\"max len \" , maxlen)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим пустые массивы нужной размерности что бы после заполнить их данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((totalEntries , maxlen, len(chars) ), dtype=np.bool)\n",
    "y = np.zeros((totalEntries , 2 ), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним данными подготовленные массивы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, sex) in enumerate(X_train):\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, int(float(sex))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5824/5824 [==============================] - 86s 15ms/step - loss: 0.2894\n",
      "Epoch 2/20\n",
      "5824/5824 [==============================] - 86s 15ms/step - loss: 0.2125\n",
      "Epoch 3/20\n",
      "5824/5824 [==============================] - 89s 15ms/step - loss: 0.1927\n",
      "Epoch 4/20\n",
      "5824/5824 [==============================] - 83s 14ms/step - loss: 0.1836\n",
      "Epoch 5/20\n",
      "5824/5824 [==============================] - 82s 14ms/step - loss: 0.1711\n",
      "Epoch 6/20\n",
      "5824/5824 [==============================] - 81s 14ms/step - loss: 0.1725\n",
      "Epoch 7/20\n",
      "5824/5824 [==============================] - 82s 14ms/step - loss: 0.1474\n",
      "Epoch 8/20\n",
      "5824/5824 [==============================] - 82s 14ms/step - loss: 0.1455\n",
      "Epoch 9/20\n",
      "5824/5824 [==============================] - 80s 14ms/step - loss: 0.1391\n",
      "Epoch 10/20\n",
      "5824/5824 [==============================] - 81s 14ms/step - loss: 0.1331\n",
      "Epoch 11/20\n",
      "5824/5824 [==============================] - 80s 14ms/step - loss: 0.1192\n",
      "Epoch 12/20\n",
      "5824/5824 [==============================] - 80s 14ms/step - loss: 0.1085\n",
      "Epoch 13/20\n",
      "5824/5824 [==============================] - 80s 14ms/step - loss: 0.0994\n",
      "Epoch 14/20\n",
      "5824/5824 [==============================] - 81s 14ms/step - loss: 0.0941\n",
      "Epoch 15/20\n",
      "5824/5824 [==============================] - 80s 14ms/step - loss: 0.0803\n",
      "Epoch 16/20\n",
      "5824/5824 [==============================] - 707s 121ms/step - loss: 0.0791\n",
      "Epoch 17/20\n",
      "5824/5824 [==============================] - 76s 13ms/step - loss: 0.0706\n",
      "Epoch 18/20\n",
      "5824/5824 [==============================] - 77s 13ms/step - loss: 0.0686\n",
      "Epoch 19/20\n",
      "5824/5824 [==============================] - 78s 13ms/step - loss: 0.0613\n",
      "Epoch 20/20\n",
      "5824/5824 [==============================] - 78s 13ms/step - loss: 0.0496\n",
      "5824/5824 [==============================] - 25s 4ms/step\n",
      "score  0.040083218793020375\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=16, epochs=20)\n",
    "model.save_weights('my_model_weights.h5')\n",
    "score = model.evaluate(X, y, batch_size=16)\n",
    "print(\"score \" , score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем максимальные значения для того что бы понять размерности входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEntries_t = X_test.shape[0]\n",
    "maxlen_t = len(max( X_test[:, 0] , key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.zeros((totalEntries_t , maxlen_t-1, len(chars) ), dtype=np.bool)\n",
    "y_t = np.zeros((totalEntries_t , 2 ), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не найден символ y в имени: Bekovich-Cherkassky\n"
     ]
    }
   ],
   "source": [
    "for i, (name, sex) in enumerate(X_test):\n",
    "    for t, char in enumerate(name):\n",
    "        try:\n",
    "            X_t[i, t, char_indices[char]] = 1\n",
    "        except:\n",
    "            print(\"Не найден символ {} в имени: {}\".format(char, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458, 18, 54)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict_classes(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tt = X_test[:,1].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.9561\n",
      "micro F1=0.9561\n",
      "macro F1=0.9561\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы: нейронная сеть дольше обучается но работает лучше. А вот с n-граммами у меня что-то пошло не так. На n=3, результат лучше чем на n=4. Хотя по идеи, чем больше размерность n-грамм, тем лучше результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 4. Генерация фамилии [35 баллов]\n",
    "\n",
    "Используйте архитектуру нейронной сети из Части 3 для генерации имени. В этот момент можно забыть про разбиение коллекции на обучающее и тестовое множество. \n",
    "\n",
    "Рассмотрите два сценария генерация имени:\n",
    "* обучение нейронной сети по всей коллекции\n",
    "* обучение нейронной сети с обуславливанием на язык\n",
    "\n",
    "Для обуславливания на язык нужно изменить векторное представление фамилии. До этого момента мы разбивали фамилию на отдельные символы и находили векторное представление каждого символа. Теперь добавим в начало фамилии метку языка и будем ее считать первым символов фамилии. Так нейронная сеть научиться понимать, на каком языке написана фамилия. \n",
    "\n",
    "Пример: ```[rus bos i v a n o v eos pad pad pad]```\n",
    "\n",
    "Когда будем генерировать новую фамилию, будем начинать процесс генерации не с символа начала последовательности ```bos```, а с символа языка и символа начала последовательности ```rus bos```. \n",
    "\n",
    "Привидите несколько примеров удачно сгенерированных фамилий. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
